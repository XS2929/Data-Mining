{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This assignment may be worked individually or in pairs. \n",
    "## Enter your name/names here:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#BULBASAUR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Assignment 1: Intro to Classification\n",
    "\n",
    "In this assignment we'll be looking at 3 common classification algorithms -- Decision Trees, k Nearest Neighbor and Naive Bayes classifier. For this task we'll be using the Diabetic Retinopathy data set, which contains features from the Messidor image set to predict whether an image contains signs of diabetic retinopathy or not. This dataset has `1151` instances and `20` attributes (some categorical, some continuous). You can find additional details about the dataset [here](http://archive.ics.uci.edu/ml/datasets/Diabetic+Retinopathy+Debrecen+Data+Set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Decision Trees\n",
    "\n",
    "For this task you'll be implementing the decision tree classifier. A few function prototypes are already given to you, please don't change those. You can add additional helper functions for your convenience. *Suggestion:* The dataset is substantially big, for the purpose of easy debugging work with a subset of the data and test your decision tree implementation on that.\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "0) The binary result of quality assessment. 0 = bad quality 1 = sufficient quality.\n",
    "\n",
    "1) The binary result of pre-screening, where 1 indicates severe retinal abnormality and 0 its lack. \n",
    "\n",
    "2-7) The results of MA detection. Each feature value stand for the number of MAs found at the confidence levels alpha = 0.5, . . . , 1, respectively. \n",
    "\n",
    "8-15) contain the same information as 2-7) for exudates. However, as exudates are represented by a set of points rather than the number of pixels constructing the lesions, these features are normalized by dividing the \n",
    "number of lesions with the diameter of the ROI to compensate different image sizes. \n",
    "\n",
    "16) The euclidean distance of the center of the macula and the center of the optic disc to provide important information regarding the patient's condition. This feature is also normalized with the diameter of the ROI.\n",
    "\n",
    "17) The diameter of the optic disc. \n",
    "\n",
    "18) The binary result of the AM/FM-based classification.\n",
    "\n",
    "19) Class label. 1 = contains signs of Diabetic Retinopathy (Accumulative label for the Messidor classes 1, 2, 3), 0 = no signs of Diabetic Retinopathy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standard Headers\n",
    "# You are welcome to add additional headers if you wish\n",
    "# EXCEPT for scikit-learn... You may NOT use scikit-learn for this assignment!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import log, pow, sqrt\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataPoint:\n",
    "    def __str__(self):\n",
    "        return \"< \" + str(self.label) + \": \" + str(self.features) + \" >\"\n",
    "    def __init__(self, label, features):\n",
    "        self.label = label # the classification label of this data point\n",
    "        self.features = features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Read data from a CSV file. Put it into a list of `DataPoints`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(filename):\n",
    "    data = []\n",
    "    global pd\n",
    "    #store the data in temp list\n",
    "    temp = pd.read_csv(filename,header=None)\n",
    "    #loop through the temp data, store them in a list of DataPoint objects\n",
    "    for i in range(len(temp)):\n",
    "        newDataPoint = DataPoint(int(temp.iloc[i][len(temp.columns)-1]),list(temp.iloc[i][:-1]))\n",
    "        data.append(newDataPoint)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    is_leaf = True          # boolean variable to check if the node is a leaf\n",
    "    feature_idx = None      # index that identifies the feature\n",
    "    thresh_val = None       # threshold value that splits the node\n",
    "    prediction = None       # prediction class (only valid for leaf nodes)\n",
    "    left_child = None       # left TreeNode (all values < thresh_val)\n",
    "    right_child = None      # right TreeNode (all values >= thresh_val)\n",
    "    \n",
    "    def printTree(self):    # for debugging purposes\n",
    "        if self.is_leaf:\n",
    "            print ('Leaf Node:      predicts ' + str(self.prediction))\n",
    "        else:\n",
    "            print ('Internal Node:  splits on feature ' \n",
    "                   + str(self.feature_idx) + ' with threshold ' + str(self.thresh_val))\n",
    "            self.left_child.printTree()\n",
    "            self.right_child.printTree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Implement the function `make_prediction` that takes the decision tree root and a `DataPoint` instance and returns the prediction label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_prediction(tree_root, data_point):\n",
    "    #if tree_root is a leaf, return class directly\n",
    "    if tree_root.is_leaf:\n",
    "        return tree_root.prediction\n",
    "    #if tree_root is not a leaf, check threshhold to decide go left or right\n",
    "    else:\n",
    "        if data_point.features[tree_root.feature_idx] <= tree_root.thresh_val:\n",
    "            return make_prediction(tree_root.left_child, data_point)\n",
    "        else:\n",
    "            return make_prediction(tree_root.right_child, data_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Implement the function `split_dataset` given an input data set, a `feature_idx` and the `threshold` for the feature. `left_split` will have all values < `threshold` and `right_split` will have all values >= `threshold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_dataset(data, feature_idx, threshold):\n",
    "    left_split = []\n",
    "    right_split = []\n",
    "    #loop through the dataset, seperate them to 2 lists\n",
    "    for i in range(len(data)):\n",
    "        if data[i].features[feature_idx] < threshold:\n",
    "            left_split.append(data[i])\n",
    "        else:\n",
    "            right_split.append(data[i])\n",
    "    return (left_split, right_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Implement the function `calc_entropy` to return the entropy of the input dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_entropy(data):\n",
    "    global log\n",
    "    entropy = 0.0\n",
    "    zero_count = 0;\n",
    "    total = len(data)\n",
    "    #count how many records have class value 0\n",
    "    for i in range(total):\n",
    "        if data[i].label == 0:\n",
    "            zero_count += 1;\n",
    "    #calculate how many record have clss value 1\n",
    "    one_count = total - zero_count\n",
    "    #possibility calculation\n",
    "    p_zero = zero_count/total\n",
    "    p_one = one_count/total\n",
    "    #change values from 0 to 1 for log calcualtion\n",
    "    if p_zero == 0:\n",
    "        p_zero = 1\n",
    "    if p_one == 0:\n",
    "        p_one = 1\n",
    "    #calculate final entropy result\n",
    "    entropy = - p_one*log(p_one,2) - p_zero*log(p_zero,2)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Implement the function `calc_best_threshold` which returns the best information gain and the corresponding threshold value for one feature at `feature_idx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_best_threshold(data, feature_idx):\n",
    "    best_info_gain = 0.0\n",
    "    best_thresh = None\n",
    "    gain = 0.0\n",
    "    #sort the data records to asending order, for threshhold finding convinience\n",
    "    newData = sorted(data, key=lambda x: x.features[feature_idx], reverse=False)\n",
    "    p_entropy = calc_entropy(newData)\n",
    "    for i in range(len(newData)-1):\n",
    "        #when finding a class split point, calculate the gain value\n",
    "        if newData[i].label != newData[i+1].label:\n",
    "            gain = p_entropy - len(newData[:i+1])*calc_entropy(newData[:i+1])/len(newData)-len(newData[i+1:])*calc_entropy(newData[i+1:])/len(newData)  \n",
    "            #every time better gain value found, update best_info_gain and best_thresh\n",
    "            if gain >= best_info_gain:\n",
    "                best_info_gain = gain\n",
    "                best_thresh = newData[i].features[feature_idx]\n",
    "    return (best_info_gain, best_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. Implement the function `identify_best_split` which returns the best feature to split on for an input dataset, and also returns the corresponding threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def identify_best_split(data):\n",
    "    if len(data) < 2:\n",
    "        return (None, None)\n",
    "    best_feature = None\n",
    "    best_thresh = None\n",
    "    best_gain = 0\n",
    "    for i in range(len(data[0].features)):\n",
    "        gain,thresh = calc_best_threshold(data,i)\n",
    "        if gain > best_gain:\n",
    "            best_gain = gain\n",
    "            best_feature = i\n",
    "            best_thresh = thresh\n",
    "    return (best_feature, best_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Implement the function `createLeafNode` which returns a `TreeNode` with `is_leaf=True` and `prediction` set to whichever classification occurs most in the dataset at this node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createLeafNode(data):\n",
    "    zero_count = 0\n",
    "    predict = 1\n",
    "    for i in range(len(data)):\n",
    "        if data[i].label == 0:\n",
    "            zero_count += 1;\n",
    "    if zero_count > len(data)/2:\n",
    "        predict = 0        \n",
    "    NewTreeNode = TreeNode()\n",
    "    NewTreeNode.prediction = predict\n",
    "    return NewTreeNode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. Implement the `createDecisionTree` function. `max_levels` denotes the maximum height of the tree (for example if `max_levels = 1` then the decision tree will only contain the leaf node at the root. [Hint: this is where the recursion happens.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createDecisionTree(data, max_levels):\n",
    "    if max_levels == 1:\n",
    "        return createLeafNode(data) \n",
    "    else:\n",
    "        #find the best feature to split\n",
    "        best_feature, best_thresh = identify_best_split(data)\n",
    "        if best_feature == None:\n",
    "            return createLeafNode(data)\n",
    "        NewTreeNode = TreeNode()\n",
    "        #when split feature found, split the data set to 2 parts according to threshhold value\n",
    "        leftSet, rightSet = split_dataset(data,best_feature,best_thresh)\n",
    "        NewTreeNode.is_leaf = False\n",
    "        #set feature index, threshhold and left, right tree node\n",
    "        NewTreeNode.feature_idx = best_feature\n",
    "        NewTreeNode.thresh_val = best_thresh\n",
    "        NewTreeNode.left_child = createDecisionTree(leftSet, max_levels-1)\n",
    "        NewTreeNode.right_child = createDecisionTree(rightSet, max_levels-1)\n",
    "    return NewTreeNode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. Given a test set, the function `calcAccuracy` returns the accuracy of the classifier. You'll use the `makePrediction` function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use global variables to store values needed for confusion matrix and precision, recall calculation\n",
    "#actual positive and predicted positive\n",
    "positive_positive = 0\n",
    "#actual positive and predicted negative\n",
    "positive_negative = 0\n",
    "#actual negative and predicted postive\n",
    "negative_positive = 0\n",
    "#actual negative and predicted negative\n",
    "negative_negative = 0\n",
    "def calcAccuracy(tree_root, data):\n",
    "    global positive_positive, positive_negative, negative_positive, negative_negative\n",
    "    positive_positive = positive_negative = negative_positive = negative_negative = 0\n",
    "    correct_count = 0\n",
    "    #loop through data for accuracy calculation and set values to global variables \n",
    "    for i in range(len(data)):\n",
    "        if make_prediction(tree_root, data[i]) == data[i].label == 1:\n",
    "            correct_count += 1\n",
    "            positive_positive += 1\n",
    "        elif  make_prediction(tree_root, data[i]) == data[i].label == 0:\n",
    "            correct_count += 1\n",
    "            negative_negative += 1\n",
    "        elif make_prediction(tree_root, data[i]) != data[i].label and data[i].label ==1:\n",
    "            positive_negative += 1\n",
    "        else:\n",
    "            negative_positive += 1\n",
    "    return correct_count/len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10. Keeping the `max_levels` parameter as 10, use 5-fold cross validation to measure the accuracy of the model. Print the recall and precision of the model. Also display the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 921\n",
      "Test set size    : 230\n",
      "Time taken: 5.492665767669678\n",
      "lable value 1 as positive and value 0 as negative\n",
      "\n",
      "\tPredicted   Class\n",
      "\t   +          -  \n",
      "--------------------------\n",
      "Actual +   60        68\n",
      "Class  -   22        80 \n",
      "\n",
      "Precision(+) = 73.17073170731707 %\n",
      "Precision(-) = 54.054054054054056 %\n",
      "Recall(+) = 46.875 %\n",
      "Recall(+) = 78.43137254901961 %\n",
      "The accuracy on the test set is  60.86956521739131 \n",
      "\n",
      "Training set size: 921\n",
      "Test set size    : 230\n",
      "Time taken: 4.9088380336761475\n",
      "lable value 1 as positive and value 0 as negative\n",
      "\n",
      "\tPredicted   Class\n",
      "\t   +          -  \n",
      "--------------------------\n",
      "Actual +   100        23\n",
      "Class  -   58        49 \n",
      "\n",
      "Precision(+) = 63.29113924050633 %\n",
      "Precision(-) = 68.05555555555556 %\n",
      "Recall(+) = 81.30081300813008 %\n",
      "Recall(+) = 45.794392523364486 %\n",
      "The accuracy on the test set is  64.78260869565217 \n",
      "\n",
      "Training set size: 921\n",
      "Test set size    : 230\n",
      "Time taken: 6.143146991729736\n",
      "lable value 1 as positive and value 0 as negative\n",
      "\n",
      "\tPredicted   Class\n",
      "\t   +          -  \n",
      "--------------------------\n",
      "Actual +   60        60\n",
      "Class  -   22        88 \n",
      "\n",
      "Precision(+) = 73.17073170731707 %\n",
      "Precision(-) = 59.45945945945946 %\n",
      "Recall(+) = 50.0 %\n",
      "Recall(+) = 80.0 %\n",
      "The accuracy on the test set is  64.34782608695652 \n",
      "\n",
      "Training set size: 921\n",
      "Test set size    : 230\n",
      "Time taken: 5.201914072036743\n",
      "lable value 1 as positive and value 0 as negative\n",
      "\n",
      "\tPredicted   Class\n",
      "\t   +          -  \n",
      "--------------------------\n",
      "Actual +   66        58\n",
      "Class  -   25        81 \n",
      "\n",
      "Precision(+) = 72.52747252747253 %\n",
      "Precision(-) = 58.273381294964025 %\n",
      "Recall(+) = 53.225806451612904 %\n",
      "Recall(+) = 76.41509433962264 %\n",
      "The accuracy on the test set is  63.91304347826087 \n",
      "\n",
      "Training set size: 921\n",
      "Test set size    : 230\n",
      "Time taken: 4.687873125076294\n",
      "lable value 1 as positive and value 0 as negative\n",
      "\n",
      "\tPredicted   Class\n",
      "\t   +          -  \n",
      "--------------------------\n",
      "Actual +   83        33\n",
      "Class  -   53        61 \n",
      "\n",
      "Precision(+) = 61.029411764705884 %\n",
      "Precision(-) = 64.8936170212766 %\n",
      "Recall(+) = 71.55172413793103 %\n",
      "Recall(+) = 53.50877192982456 %\n",
      "The accuracy on the test set is  62.60869565217392 \n",
      "\n",
      "\n",
      "Average accuracy: 63.30434782608695\n"
     ]
    }
   ],
   "source": [
    "# edit the code here - this is just a sample to get you started\n",
    "import time\n",
    "\n",
    "d = get_data(\"messidor_features.txt\")\n",
    "test_size = len(d)//5\n",
    "accuracy_record = []\n",
    "for i in range(5):\n",
    "    # partition data into train_set and test_set\n",
    "    train_set = d[:i*test_size]+d[(i+1)*test_size:]\n",
    "    test_set = d[i*test_size:(i+1)*test_size]\n",
    "\n",
    "    print ('Training set size:', len(train_set))\n",
    "    print ('Test set size    :', len(test_set))\n",
    "\n",
    "    # create the decision tree\n",
    "    start = time.time()\n",
    "    tree = createDecisionTree(train_set, 10)\n",
    "    end = time.time()\n",
    "    print ('Time taken:', end - start)\n",
    "\n",
    "    # calculate the accuracy of the tree\n",
    "    accuracy = calcAccuracy(tree, test_set)\n",
    "    accuracy_record.append(accuracy)\n",
    "    #print confusion matrix and calculate Precision, Recall values\n",
    "    print('lable value 1 as positive and value 0 as negative')\n",
    "    print('\\n\\tPredicted   Class')\n",
    "    print('\\t   +          -  ')\n",
    "    print('--------------------------')\n",
    "    print('Actual +  ',positive_positive,'      ',positive_negative)\n",
    "    print('Class  -  ',negative_positive,'      ',negative_negative,'\\n')\n",
    "    print('Precision(+) =',100*positive_positive/(positive_positive+negative_positive),'%')\n",
    "    print('Precision(-) =',100*negative_negative/(negative_negative+positive_negative),'%')\n",
    "    print('Recall(+) =',100*positive_positive/(positive_positive+positive_negative),'%')\n",
    "    print('Recall(+) =',100*negative_negative/(negative_negative+negative_positive),'%')    \n",
    "    print ('The accuracy on the test set is ', str(accuracy * 100.0),'\\n')\n",
    "print('\\nAverage accuracy:',100*sum(accuracy_record)/len(accuracy_record))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q11. Extra Credit: Implement a pruning algorithm on the decision tree (either chi-squared, reduced error pruning, or model selection using a validation set/validation error) and see if that improves the generalization error of the decision tree (using 5-fold CV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: KNN Classifier/Naive Bayes Classifier\n",
    "\n",
    "For this task you have an option to either implement the k Nearest Neighbor classifier or the Naive Bayes classifier. You will be using the same dataset as above. The implementation details are up to you but, it is generally a good idea to divide your code into helper functions.\n",
    "\n",
    "For your implemented model, measure the accuracy of the KNN/NB classifier using 5-fold cross validation. Compare this to the decision tree model you created above. Also print the precision and recall of the KNN/NB classifier and display the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 921\n",
      "Test set size    : 230\n",
      "Time taken: 6.797586917877197\n",
      "The accuracy on the test set is  66.08695652173913 \n",
      "\n",
      "lable value 1 as positive and value 0 as negative\n",
      "\n",
      "\tPredicted   Class\n",
      "\t   +          -  \n",
      "--------------------------\n",
      "Actual +   79        49\n",
      "Class  -   29        73 \n",
      "\n",
      "Precision(+) = 73.14814814814815 %\n",
      "Precision(-) = 59.83606557377049 %\n",
      "Recall(+) = 61.71875 %\n",
      "Recall(+) = 71.56862745098039 %\n",
      "The accuracy on the test set is  66.08695652173913 \n",
      "\n",
      "Training set size: 921\n",
      "Test set size    : 230\n",
      "Time taken: 6.135332822799683\n",
      "The accuracy on the test set is  62.17391304347826 \n",
      "\n",
      "lable value 1 as positive and value 0 as negative\n",
      "\n",
      "\tPredicted   Class\n",
      "\t   +          -  \n",
      "--------------------------\n",
      "Actual +   70        53\n",
      "Class  -   34        73 \n",
      "\n",
      "Precision(+) = 67.3076923076923 %\n",
      "Precision(-) = 57.93650793650794 %\n",
      "Recall(+) = 56.91056910569106 %\n",
      "Recall(+) = 68.22429906542057 %\n",
      "The accuracy on the test set is  62.17391304347826 \n",
      "\n",
      "Training set size: 921\n",
      "Test set size    : 230\n",
      "Time taken: 5.139885187149048\n",
      "The accuracy on the test set is  70.0 \n",
      "\n",
      "lable value 1 as positive and value 0 as negative\n",
      "\n",
      "\tPredicted   Class\n",
      "\t   +          -  \n",
      "--------------------------\n",
      "Actual +   81        39\n",
      "Class  -   30        80 \n",
      "\n",
      "Precision(+) = 72.97297297297297 %\n",
      "Precision(-) = 67.22689075630252 %\n",
      "Recall(+) = 67.5 %\n",
      "Recall(+) = 72.72727272727273 %\n",
      "The accuracy on the test set is  70.0 \n",
      "\n",
      "Training set size: 921\n",
      "Test set size    : 230\n",
      "Time taken: 5.4208009243011475\n",
      "The accuracy on the test set is  63.47826086956522 \n",
      "\n",
      "lable value 1 as positive and value 0 as negative\n",
      "\n",
      "\tPredicted   Class\n",
      "\t   +          -  \n",
      "--------------------------\n",
      "Actual +   76        48\n",
      "Class  -   36        70 \n",
      "\n",
      "Precision(+) = 67.85714285714286 %\n",
      "Precision(-) = 59.32203389830509 %\n",
      "Recall(+) = 61.29032258064516 %\n",
      "Recall(+) = 66.0377358490566 %\n",
      "The accuracy on the test set is  63.47826086956522 \n",
      "\n",
      "Training set size: 921\n",
      "Test set size    : 230\n",
      "Time taken: 5.779267072677612\n",
      "The accuracy on the test set is  63.47826086956522 \n",
      "\n",
      "lable value 1 as positive and value 0 as negative\n",
      "\n",
      "\tPredicted   Class\n",
      "\t   +          -  \n",
      "--------------------------\n",
      "Actual +   72        44\n",
      "Class  -   40        74 \n",
      "\n",
      "Precision(+) = 64.28571428571429 %\n",
      "Precision(-) = 62.71186440677966 %\n",
      "Recall(+) = 62.06896551724138 %\n",
      "Recall(+) = 64.91228070175438 %\n",
      "The accuracy on the test set is  63.47826086956522 \n",
      "\n",
      "\n",
      "Average accuracy: 65.04347826086956\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "function to calculate the distance square of two data records\n",
    "'''\n",
    "def calculateDistanceSquare(train_data, test_data):\n",
    "    global pow\n",
    "    #binary list stores binary attibute index\n",
    "    binary_list = [0,1,18]\n",
    "    new_distance_square = 0\n",
    "    #loop through all attributes and sum up the attribute distance square\n",
    "    for i in range(len(test_data.features)):\n",
    "        #if binary attribute, set distance 1 or 0\n",
    "        if i in binary_list:\n",
    "            if test_data.features[i] == train_data.features[i]:\n",
    "                new_distance_square += 0\n",
    "                new_distance_square\n",
    "            else:\n",
    "                new_distance_square += 1\n",
    "                new_distance_square\n",
    "        #if not binary, calculate the distance \n",
    "        else:\n",
    "            new_distance_square += pow(test_data.features[i]-train_data.features[i],2)\n",
    "    return new_distance_square\n",
    "\n",
    "'''\n",
    "Function to make class prediction for a test data record, k is the chosen neighbor number\n",
    "prediction is made by weighted voting result\n",
    "'''\n",
    "def make_KNN_prediction(train_data_set, test_data, k):\n",
    "    global sqrt\n",
    "    distance_list = []\n",
    "    #loop through the training data set, calculate each distance and weight value\n",
    "    for i in range(len(train_data_set)):\n",
    "        square = calculateDistanceSquare(train_data_set[i], test_data)\n",
    "        #if all attributes fit test data record, record weight as 1\n",
    "        if square == 0:\n",
    "            weight = 1\n",
    "        #if not, calculate weight value\n",
    "        else:\n",
    "            weight = 1/square\n",
    "        #store the new neighbor info as a list to a list of lists, [index in training set, distance, weight]\n",
    "        distance_list.append([i,sqrt(square),weight])\n",
    "    #for further using, sort the neighbors in asending order by distance\n",
    "    distance_list = sorted(distance_list, key=lambda x: x[1], reverse=False)\n",
    "    #only store the needed k neighbors to a new list\n",
    "    result_list = distance_list[:k]\n",
    "    #one_sum and zero_sum will store voting status\n",
    "    one_sum = zero_sum = 0.0\n",
    "    for i in range(len(result_list)):\n",
    "        if train_data_set[result_list[i][0]].label == 1:\n",
    "            one_sum += result_list[i][2]\n",
    "        else:\n",
    "            zero_sum += result_list[i][2]\n",
    "    #predict the label\n",
    "    if one_sum > zero_sum:\n",
    "        return 1;\n",
    "    else:\n",
    "        return 0;\n",
    "'''\n",
    "function to calculate accuracy, basically mimicing the one in decision tree\n",
    "also uses glocal variable to store the values used in cinfusion matrix and precision,recall calculating\n",
    "'''    \n",
    "def calculate_KNN_accuracy(train_data_set,test_data_set, k):\n",
    "    global positive_positive, positive_negative, negative_positive, negative_negative\n",
    "    positive_positive = positive_negative = negative_positive = negative_negative = 0\n",
    "    correct_count = 0;\n",
    "    for i in range(len(test_data_set)):\n",
    "        if make_KNN_prediction(train_data_set,test_data_set[i],k) == test_data_set[i].label == 1:\n",
    "            correct_count += 1\n",
    "            positive_positive += 1\n",
    "        elif make_KNN_prediction(train_data_set,test_data_set[i],k) == test_data_set[i].label == 0:\n",
    "            correct_count += 1\n",
    "            negative_negative += 1\n",
    "        elif make_KNN_prediction(train_data_set,test_data_set[i],k) != test_data_set[i].label and test_data_set[i].label == 1:\n",
    "            positive_negative += 1\n",
    "        else:\n",
    "            negative_positive += 1\n",
    "    return correct_count/len(test_data_set)\n",
    "\n",
    "\n",
    "d = get_data(\"messidor_features.txt\")\n",
    "test_size = len(d)//5\n",
    "accuracy_record = []\n",
    "for i in range(5):\n",
    "    # partition data into train_set and test_set\n",
    "    train_set = d[:i*test_size]+d[(i+1)*test_size:]\n",
    "    test_set = d[i*test_size:(i+1)*test_size]\n",
    "\n",
    "    print ('Training set size:', len(train_set))\n",
    "    print ('Test set size    :', len(test_set))\n",
    "\n",
    "    # calculate the accuracy\n",
    "    start = time.time()\n",
    "    accuracy = calculate_KNN_accuracy(train_set, test_set, 11)\n",
    "    end = time.time()\n",
    "    print ('Time taken:', end - start)\n",
    "    print ('The accuracy on the test set is ', str(accuracy * 100.0),'\\n')\n",
    "    accuracy_record.append(accuracy)\n",
    "    print('lable value 1 as positive and value 0 as negative')\n",
    "    print('\\n\\tPredicted   Class')\n",
    "    print('\\t   +          -  ')\n",
    "    print('--------------------------')\n",
    "    print('Actual +  ',positive_positive,'      ',positive_negative)\n",
    "    print('Class  -  ',negative_positive,'      ',negative_negative,'\\n')\n",
    "    print('Precision(+) =',100*positive_positive/(positive_positive+negative_positive),'%')\n",
    "    print('Precision(-) =',100*negative_negative/(negative_negative+positive_negative),'%')\n",
    "    print('Recall(+) =',100*positive_positive/(positive_positive+positive_negative),'%')\n",
    "    print('Recall(+) =',100*negative_negative/(negative_negative+negative_positive),'%')    \n",
    "    print ('The accuracy on the test set is ', str(accuracy * 100.0),'\\n')\n",
    "print('\\nAverage accuracy:',100*sum(accuracy_record)/len(accuracy_record))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
